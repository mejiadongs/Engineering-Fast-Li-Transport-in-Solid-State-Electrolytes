{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from cgcnn.data import CIFData, collate_pool\n",
    "\n",
    "def infer_cgcnn_dims_from_sd(state_dict):\n",
    "    dims = {}\n",
    "    emb_key = \"embedding.weight\"\n",
    "    if emb_key not in state_dict:\n",
    "        raise ValueError(f\"Cannot find {emb_key} in checkpoint to infer dims.\")\n",
    "    emb_shape = state_dict[emb_key].shape  # e.g. (64, 92)\n",
    "    atom_fea_len, orig_atom_fea_len = emb_shape\n",
    "    dims[\"orig_atom_fea_len\"] = orig_atom_fea_len\n",
    "    dims[\"atom_fea_len\"] = atom_fea_len\n",
    "\n",
    "    n_conv = 0\n",
    "    conv0_key = None\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith(\"convs.\") and k.endswith(\".fc_full.weight\"):\n",
    "            n_conv += 1\n",
    "            if conv0_key is None:\n",
    "                conv0_key = k\n",
    "\n",
    "    dims[\"n_conv\"] = n_conv\n",
    "    if conv0_key is None:\n",
    "        raise ValueError(\"Cannot find any convs.X.fc_full.weight in checkpoint.\")\n",
    "    conv0_shape = state_dict[conv0_key].shape  # e.g. (128, 229)\n",
    "    out_features, in_features = conv0_shape\n",
    "    # out_features = 2*atom_fea_len => double check\n",
    "    # in_features = 2*atom_fea_len + nbr_fea_len\n",
    "    # => nbr_fea_len = in_features - out_features\n",
    "    # out_features must match 2*dims[\"atom_fea_len\"]\n",
    "    if out_features != 2 * dims[\"atom_fea_len\"]:\n",
    "        raise ValueError(f\"fc_full out_features({out_features}) != 2*atom_fea_len({2*dims['atom_fea_len']}).\")\n",
    "    nbr_fea_len = in_features - out_features\n",
    "    dims[\"nbr_fea_len\"] = nbr_fea_len\n",
    "\n",
    "    # 3) 解析 fc_out.weight => shape (out_dim, h_fea_len)\n",
    "    fc_out_key = \"fc_out.weight\"\n",
    "    if fc_out_key not in state_dict:\n",
    "        raise ValueError(\"Cannot find fc_out.weight in checkpoint.\")\n",
    "    fc_out_shape = state_dict[fc_out_key].shape  # e.g. (1, 128)\n",
    "    out_dim, h_fea_len = fc_out_shape\n",
    "    dims[\"out_dim\"] = out_dim\n",
    "    dims[\"h_fea_len\"] = h_fea_len\n",
    "\n",
    "    return (\n",
    "        dims[\"orig_atom_fea_len\"],\n",
    "        dims[\"atom_fea_len\"],\n",
    "        dims[\"nbr_fea_len\"],\n",
    "        dims[\"n_conv\"],\n",
    "        dims[\"h_fea_len\"],\n",
    "        dims[\"out_dim\"]\n",
    "    )\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# B. ModifiedConvLayer\n",
    "##############################################################################\n",
    "class ModifiedConvLayer(nn.Module):\n",
    "    def __init__(self, atom_fea_len, nbr_fea_len):\n",
    "        super().__init__()\n",
    "        self.atom_fea_len = atom_fea_len\n",
    "        self.nbr_fea_len = nbr_fea_len\n",
    "\n",
    "        in_features = 2 * atom_fea_len + nbr_fea_len\n",
    "        out_features = 2 * atom_fea_len\n",
    "        self.fc_full = nn.Linear(in_features, out_features)\n",
    "        self.bn1 = nn.BatchNorm1d(out_features)\n",
    "        self.bn2 = nn.BatchNorm1d(atom_fea_len)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softplus1 = nn.Softplus()\n",
    "        self.softplus2 = nn.Softplus()\n",
    "\n",
    "    def forward(self, atom_in_fea, nbr_fea, nbr_fea_idx):\n",
    "        N, M = nbr_fea_idx.shape\n",
    "        atom_nbr_fea = atom_in_fea[nbr_fea_idx, :]  # (N, M, atom_fea_len)\n",
    "\n",
    "        total_nbr_fea = torch.cat([\n",
    "            atom_in_fea.unsqueeze(1).expand(N, M, self.atom_fea_len),\n",
    "            atom_nbr_fea,\n",
    "            nbr_fea\n",
    "        ], dim=2)  # shape => (N, M, 2*atom_fea_len + nbr_fea_len)\n",
    "\n",
    "        gated_fea = self.fc_full(total_nbr_fea.view(-1, total_nbr_fea.shape[-1]))\n",
    "        gated_fea = self.bn1(gated_fea).view(N, M, 2*self.atom_fea_len)\n",
    "\n",
    "        nbr_filter, nbr_core = gated_fea.chunk(2, dim=2)\n",
    "        nbr_filter = self.sigmoid(nbr_filter)\n",
    "        nbr_core = self.softplus1(nbr_core)\n",
    "\n",
    "        nbr_sumed = torch.sum(nbr_filter * nbr_core, dim=1)\n",
    "        nbr_sumed = self.bn2(nbr_sumed)\n",
    "        out = self.softplus2(atom_in_fea + nbr_sumed)\n",
    "        return out\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# C. CrystalGraphConvNetWithHooks\n",
    "##############################################################################\n",
    "class CrystalGraphConvNetWithHooks(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        orig_atom_fea_len,\n",
    "        atom_fea_len,\n",
    "        nbr_fea_len,\n",
    "        n_conv,\n",
    "        h_fea_len,\n",
    "        out_dim=1,\n",
    "        classification=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.intermediate_outputs = {}\n",
    "\n",
    "        # 1) embedding\n",
    "        self.embedding = nn.Linear(orig_atom_fea_len, atom_fea_len)\n",
    "\n",
    "        # 2) n_conv layers\n",
    "        self.convs = nn.ModuleList([\n",
    "            ModifiedConvLayer(atom_fea_len, nbr_fea_len) for _ in range(n_conv)\n",
    "        ])\n",
    "\n",
    "        # 3) pooling\n",
    "        self.pooling = self._pooling_mean\n",
    "\n",
    "        # 4) conv_to_fc\n",
    "        self.conv_to_fc = nn.Linear(atom_fea_len, h_fea_len)\n",
    "        self.conv_to_fc_softplus = nn.Softplus()\n",
    "\n",
    "        # 5) fc_out\n",
    "        if classification and out_dim > 1:\n",
    "            self.fc_out = nn.Linear(h_fea_len, out_dim)\n",
    "        else:\n",
    "            self.fc_out = nn.Linear(h_fea_len, out_dim)\n",
    "\n",
    "    def _pooling_mean(self, atom_fea, crystal_atom_idx_list):\n",
    "        pooled = []\n",
    "        for idx in crystal_atom_idx_list:\n",
    "            chunk = atom_fea[idx]\n",
    "            pooled.append(chunk.mean(dim=0, keepdim=True))\n",
    "        return torch.cat(pooled, dim=0)\n",
    "\n",
    "    def add_hook(self, name):\n",
    "        def hook(module, input, output):\n",
    "            self.intermediate_outputs[name] = output.detach()\n",
    "        return hook\n",
    "\n",
    "    def forward(self, atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx):\n",
    "        self.embedding.register_forward_hook(self.add_hook(\"embedding\"))\n",
    "        for i, conv_layer in enumerate(self.convs):\n",
    "            conv_layer.register_forward_hook(self.add_hook(f\"conv_{i}\"))\n",
    "        self.fc_out.register_forward_hook(self.add_hook(\"fc_out\"))\n",
    "\n",
    "        # 1) embedding\n",
    "        atom_fea = self.embedding(atom_fea)\n",
    "        # 2) conv\n",
    "        for conv_layer in self.convs:\n",
    "            atom_fea = conv_layer(atom_fea, nbr_fea, nbr_fea_idx)\n",
    "\n",
    "        # 3) pooling\n",
    "        if isinstance(crystal_atom_idx, torch.Tensor):\n",
    "            crystal_atom_idx = [crystal_atom_idx]\n",
    "        crys_fea = self.pooling(atom_fea, crystal_atom_idx)\n",
    "\n",
    "        # 4) conv_to_fc\n",
    "        crys_fea = self.conv_to_fc_softplus(self.conv_to_fc(crys_fea))\n",
    "\n",
    "        # 5) fc_out\n",
    "        out = self.fc_out(crys_fea)\n",
    "        return out\n",
    "\n",
    "    def get_graph_embedding(self, atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx):\n",
    "        with torch.no_grad():\n",
    "            atom_fea = self.embedding(atom_fea)\n",
    "            for conv_layer in self.convs:\n",
    "                atom_fea = conv_layer(atom_fea, nbr_fea, nbr_fea_idx)\n",
    "            if isinstance(crystal_atom_idx, torch.Tensor):\n",
    "                crystal_atom_idx = [crystal_atom_idx]\n",
    "            crys_fea = self.pooling(atom_fea, crystal_atom_idx)\n",
    "            crys_fea = self.conv_to_fc_softplus(self.conv_to_fc(crys_fea))\n",
    "        return crys_fea\n",
    "\n",
    "class ModelWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, checkpoint_path):\n",
    "        super().__init__()\n",
    "        # 1) 加载 checkpoint\n",
    "        ckpt = torch.load(checkpoint_path, map_location='cpu')\n",
    "        state_dict = ckpt['state_dict'] if 'state_dict' in ckpt else ckpt\n",
    "\n",
    "        (orig_atom_fea_len,\n",
    "         atom_fea_len,\n",
    "         nbr_fea_len,\n",
    "         n_conv,\n",
    "         h_fea_len,\n",
    "         out_dim) = infer_cgcnn_dims_from_sd(state_dict)\n",
    "\n",
    "        print(\"[ModelWrapper] Inferred dims =>\",\n",
    "              f\"orig_atom_fea_len={orig_atom_fea_len},\",\n",
    "              f\"atom_fea_len={atom_fea_len},\",\n",
    "              f\"nbr_fea_len={nbr_fea_len},\",\n",
    "              f\"n_conv={n_conv},\",\n",
    "              f\"h_fea_len={h_fea_len},\",\n",
    "              f\"out_dim={out_dim}\"\n",
    "        )\n",
    "\n",
    "        classification = (out_dim > 1)\n",
    "        self.model = CrystalGraphConvNetWithHooks(\n",
    "            orig_atom_fea_len=orig_atom_fea_len,\n",
    "            atom_fea_len=atom_fea_len,\n",
    "            nbr_fea_len=nbr_fea_len,\n",
    "            n_conv=n_conv,\n",
    "            h_fea_len=h_fea_len,\n",
    "            out_dim=out_dim,\n",
    "            classification=classification\n",
    "        )\n",
    "\n",
    "        self.model.load_state_dict(state_dict, strict=True)\n",
    "        self.model.eval()\n",
    "\n",
    "    def forward(self, graph_batch):\n",
    "        atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx = graph_batch\n",
    "        return self.model.get_graph_embedding(atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx)\n",
    "\n",
    "def visualize_layer_output(layer_name, output):\n",
    "    arr = output.squeeze().cpu().numpy()\n",
    "    plt.figure(figsize=(8, 4))\n",
    "\n",
    "    if arr.ndim == 0:\n",
    "        plt.text(0.5, 0.5, f\"{layer_name} scalar: {arr:.4f}\",\n",
    "                 ha='center', va='center', fontsize=14)\n",
    "        plt.axis('off')\n",
    "    elif arr.ndim == 1:\n",
    "        plt.plot(arr, marker='o')\n",
    "        plt.title(f\"{layer_name} (1D)\")\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel('Value')\n",
    "    elif arr.ndim == 2:\n",
    "        plt.imshow(arr, aspect='auto', cmap='viridis')\n",
    "        plt.colorbar()\n",
    "        plt.title(f\"{layer_name} (2D)\")\n",
    "        plt.xlabel('Feature dim')\n",
    "        plt.ylabel('Sample index')\n",
    "    else:\n",
    "        print(f\"[visualize_layer_output] shape {arr.shape} not easily visualizable.\")\n",
    "        plt.close()\n",
    "        return\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{layer_name}_output.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def analyze_feature_activation(layer_name, output):\n",
    "    arr = output.squeeze().cpu().numpy()\n",
    "    if arr.ndim != 2:\n",
    "        print(f\"[analyze_feature_activation] skip, arr.ndim={arr.ndim}\")\n",
    "        return\n",
    "\n",
    "    mean_act = np.mean(arr, axis=0)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.bar(range(len(mean_act)), mean_act)\n",
    "    plt.title(f\"{layer_name} Mean Activation\")\n",
    "    plt.xlabel('Channel index')\n",
    "    plt.ylabel('Mean')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{layer_name}_mean_activation.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def dissect_single_sample(model_ckpt_path, data_path):\n",
    "    print(\"\\n=== [dissect_single_sample] ===\")\n",
    "\n",
    "    ckpt = torch.load(model_ckpt_path, map_location='cpu')\n",
    "    state_dict = ckpt['state_dict'] if 'state_dict' in ckpt else ckpt\n",
    "\n",
    "    (orig_atom_fea_len,\n",
    "     atom_fea_len,\n",
    "     nbr_fea_len,\n",
    "     n_conv,\n",
    "     h_fea_len,\n",
    "     out_dim) = infer_cgcnn_dims_from_sd(state_dict)\n",
    "\n",
    "    classification = (out_dim > 1)\n",
    "    model = CrystalGraphConvNetWithHooks(\n",
    "        orig_atom_fea_len=orig_atom_fea_len,\n",
    "        atom_fea_len=atom_fea_len,\n",
    "        nbr_fea_len=nbr_fea_len,\n",
    "        n_conv=n_conv,\n",
    "        h_fea_len=h_fea_len,\n",
    "        out_dim=out_dim,\n",
    "        classification=classification\n",
    "    )\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    model.eval()\n",
    "\n",
    "    print(\"[dissect_single_sample] =>\",\n",
    "          f\"orig_atom_fea_len={orig_atom_fea_len}, atom_fea_len={atom_fea_len},\",\n",
    "          f\"nbr_fea_len={nbr_fea_len}, n_conv={n_conv}, h_fea_len={h_fea_len}, out_dim={out_dim}\")\n",
    "\n",
    "\n",
    "    dataset = CIFData(data_path, radius=20.0)\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collate_pool)\n",
    "    sample_input = next(iter(loader))\n",
    "    graph_inputs, target, cif_id = sample_input\n",
    "\n",
    "\n",
    "    if len(graph_inputs) == 3:\n",
    "        atom_fea, nbr_fea, crystal_atom_idx = graph_inputs\n",
    "        nbr_fea_idx = torch.arange(atom_fea.shape[0]).unsqueeze(-1)\n",
    "        graph_inputs = (atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx)\n",
    "\n",
    "    # 4) forward\n",
    "    with torch.no_grad():\n",
    "        output = model(*graph_inputs)\n",
    "    print(\"[dissect_single_sample] forward done. Output shape:\", output.shape)\n",
    "    if output.numel() == 1:\n",
    "        print(\"Model output value:\", output.item())\n",
    "    else:\n",
    "        print(\"Model output (batch):\", output)\n",
    "\n",
    "    # 5) 可视化中间层\n",
    "    print(\"\\n[dissect_single_sample] Intermediate outputs:\")\n",
    "    for name, out_tensor in model.intermediate_outputs.items():\n",
    "        print(f\"  {name}: shape={tuple(out_tensor.shape)}\")\n",
    "        visualize_layer_output(name, out_tensor)\n",
    "        analyze_feature_activation(name, out_tensor)\n",
    "\n",
    "\n",
    "def run_shap_analysis(model_ckpt_path, data_path):\n",
    "    print(\"\\n=== [run_shap_analysis] ===\")\n",
    "    emb_model = ModelWrapper(model_ckpt_path)\n",
    "    emb_model.eval()\n",
    "\n",
    "    # dataset\n",
    "    dataset = CIFData(data_path, radius=20.0)\n",
    "    loader = DataLoader(dataset, batch_size=16, shuffle=False, collate_fn=collate_pool)\n",
    "\n",
    "    X_emb_list, y_list = [], []\n",
    "    for batch in loader:\n",
    "        graph_inputs, targets, cif_ids = batch\n",
    "        if len(graph_inputs) == 3:\n",
    "            atom_fea, nbr_fea, crystal_atom_idx = graph_inputs\n",
    "            nbr_fea_idx = torch.arange(atom_fea.shape[0]).unsqueeze(-1)\n",
    "            graph_inputs = (atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx)\n",
    "        emb = emb_model(graph_inputs).cpu()\n",
    "        X_emb_list.append(emb)\n",
    "        y_list.append(targets)\n",
    "\n",
    "    X_emb = torch.cat(X_emb_list, dim=0)\n",
    "    y_test = torch.cat(y_list).numpy()\n",
    "    print(\"Graph embedding shape:\", X_emb.shape)\n",
    "    print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "    ckpt = torch.load(model_ckpt_path, map_location='cpu')\n",
    "    state_dict = ckpt['state_dict'] if 'state_dict' in ckpt else ckpt\n",
    "    fc_w = state_dict['fc_out.weight'].cpu()\n",
    "    fc_b = state_dict['fc_out.bias'].cpu()\n",
    "\n",
    "    def predict_from_emb(X_numpy):\n",
    "        X_t = torch.tensor(X_numpy, dtype=torch.float32)\n",
    "        out = X_t @ fc_w.T + fc_b\n",
    "        return out.squeeze(-1).detach().numpy()\n",
    "\n",
    "    # shap\n",
    "    background_size = min(200, X_emb.shape[0])\n",
    "    background_data = X_emb[:background_size].numpy()\n",
    "\n",
    "    explainer = shap.KernelExplainer(predict_from_emb, background_data)\n",
    "    sample_size = min(200, X_emb.shape[0])\n",
    "    sample_data = X_emb[:sample_size].numpy()\n",
    "\n",
    "    shap_values = explainer.shap_values(sample_data)\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[0]\n",
    "    shap_values = np.array(shap_values).reshape(sample_size, -1)\n",
    "    print(\"SHAP values shape:\", shap_values.shape)\n",
    "\n",
    "    emb_dim = shap_values.shape[1]\n",
    "    feature_names = [f\"GNN_Emb_{i}\" for i in range(emb_dim)]\n",
    "\n",
    "    # summary plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_values, sample_data, feature_names=feature_names,\n",
    "                      plot_type='dot', max_display=10, show=False)\n",
    "    plt.savefig('shap_summary_swarm.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"[SHAP] summary swarm plot saved -> shap_summary_swarm.png\")\n",
    "\n",
    "    # top K\n",
    "    mean_importance = np.mean(np.abs(shap_values), axis=0)\n",
    "    sorted_idx = np.argsort(mean_importance)\n",
    "    top_k = 10\n",
    "    print(f\"\\nTop {top_k} important embedding dims:\")\n",
    "    for i in range(1, top_k + 1):\n",
    "        idx = sorted_idx[-i]\n",
    "        print(f\"  {feature_names[idx]} = {mean_importance[idx]:.4f}\")\n",
    "\n",
    "    # save shap\n",
    "    df_shap = pd.DataFrame(shap_values, columns=feature_names)\n",
    "    df_shap.to_csv('shap_values_emb.csv', index=False)\n",
    "    print(\"[SHAP] saved shap_values_emb.csv\")\n",
    "\n",
    "    df_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': mean_importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    df_importance.to_csv('feature_importance_emb.csv', index=False)\n",
    "    print(\"[SHAP] saved feature_importance_emb.csv\")\n",
    "\n",
    "    df_emb_for_shap = pd.DataFrame(sample_data, columns=feature_names)\n",
    "    # 如果想带上对应的目标值\n",
    "    df_emb_for_shap[\"target\"] = y_test[:sample_size]\n",
    "    df_emb_for_shap.to_csv(\"shap_input_emb.csv\", index=False)\n",
    "    print(\"[SHAP] saved shap_input_emb.csv (the embedding input to shap).\")\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# H. main\n",
    "##############################################################################\n",
    "def main():\n",
    "    model_ckpt_path = r\"model_best.pth.tar\"\n",
    "    data_path = r\"\"\n",
    "\n",
    "    # 1) 对单样本做卷积层可视化\n",
    "    dissect_single_sample(model_ckpt_path, data_path)\n",
    "\n",
    "    # 2) 对全数据做 SHAP\n",
    "    run_shap_analysis(model_ckpt_path, data_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cgcnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
